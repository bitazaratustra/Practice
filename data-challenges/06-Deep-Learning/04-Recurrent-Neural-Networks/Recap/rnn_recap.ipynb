{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã Welcome to this recap of the RNN unit!\n",
    "\n",
    "üìÜ We will be revisiting *Challenge 3 - Predict Weather Temperature*.\n",
    "\n",
    "\n",
    "üéØ ***Predict one target (`N_TARGETS = 1`)***: \n",
    "- üå° `T (degC)` = temperature in Celsius degrees\n",
    "\n",
    "\n",
    "We will introduce two modifications to this challenge:\n",
    "\n",
    "* (1) üÜï an `OUTPUT_LENGTH > 1`\n",
    "    * $ \\Leftrightarrow$ we will try to predict <u>***MULTIPLE time steps in the future***</u>\n",
    "    \n",
    "    \n",
    "\n",
    "* (2) üÜï a `HORIZON ‚â•1` between the last seen value in $X$ and the first value $y$ to predict in the future:\n",
    "    * $ \\Leftrightarrow$ we want to <u>***make the predictions a few moments AFTER the last known values***</u> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# Checking data types\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) üìö The weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üå¶ For this recap, we will re-use the preprocessed weather dataset from the *challenge 3 - Predict Weather Temperature*. \n",
    "\n",
    "\n",
    "üíæ Load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/weather_every_three_hours_engineered.csv\"\n",
    "df = pd.read_csv(url).drop(columns = ['Unnamed: 0'])\n",
    "display(df.tail())\n",
    "print(f\"df.shape = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ *(Reminders*) This weather dataset is a ***single Time Series*** represented as a DataFrame, i.e. a **2D-array**.\n",
    "- `df.shape = (n_timesteps, n_features)`\n",
    "    - `n_timesteps` $= 23$k rows  (_~8 years of weather data, from 2009 to 2016 with records every 3 hours_)\n",
    "    - `n_features` $= 19$ features composed of:\n",
    "        - $1$ <font color=green>**target**</font> (we will use the past values of the temperature as a feature)\n",
    "        - $18$ <font color=orange>**past covariates**</font> (= features which past values are known)\n",
    "        - $0$ <font color=blue>**future covariates**</font> (= features which future values are known, e.g. public holidays)\n",
    "    \n",
    "<img src='https://github.com/lewagon/data-images/blob/master/DL/time-series-covariates.png?raw=true'>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) üåÖ The big picture about dealing with Time Series *(reminder)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ <b>[FOLDS] <u>Cross-Validation in Time Series  </u></b>\n",
    "\n",
    "Starting from this single Time Series:\n",
    "- We will create <font color=\"#c91ac9\">**FOLDS**</font>\n",
    "- <font color=blue>**Train**</font>/<font color=\"#ff8005\">**Evaluate**</font> our LSTM  <font color=\"#c91ac9\">**on each of these different FOLDS**</font> to conclude about <b><u>the robustness of the model</u><b>.\n",
    "    \n",
    "_It is very common to create hundreds of folds in Time Series forecasting, in order to cover all types of external conditions: crash market periods, bull markets, atone markets, etc..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ <b>[TRAIN-TEST SPLIT] <u>Holdout method</u></b>\n",
    "\n",
    "For each <font color=\"#c91ac9\">**FOLD**</font>, we will do a <font color=blue>**TRAIN**</font>-<font color=\"#ff8005\">**TEST**</font> SPLIT to:\n",
    "* <font color=blue>**fit**</font> the model on the <font color=blue>**train**</font> set \n",
    "* <font color=\"#ff8005\">**evaluate**</font> it on the <font color=\"#ff8005\">**test**</font> set\n",
    "\n",
    "_Always split the train set **chronologically** before the test set!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "üëá The first two steps can be summarized in the following image (here, we illustrated a 4-fold temporal cross-validation):\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/time_series_cross_validation.png\" alt=\"Time Series Cross Validation\" width=\"800\" height=\"400\">\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ <b>[SEQUENCES] <u>Sampling/Extracting sequences</u></b>\n",
    "\n",
    "\n",
    "After splitting each fold into a <font color=\"blue\">train</font> set and a <font color=\"#ff8005\">test</font> set, it is time to:\n",
    "- üèã sample lots of <font color=\"#884dff\"><i>sequences</i></font> $\\color{blue}{(X_i, y_i)}$ on which the model will be <font color=\"blue\">trained</font>\n",
    "- üë©üèª‚Äçüè´ sample lots of <font color=\"#884dff\"><i>sequences</i></font> $\\color{#ff8005}{(X_i, y_i)}$ on which the model will be <font color=\"#ff8005\">evaluated</font>\n",
    "\n",
    "\n",
    "<img src=\"https://bit.ly/3Ri8Vfd\" alt=\"Sequences in each fold\" width=\"500\" height=\"500\"> \n",
    "\n",
    "üëâ All these <font color=\"#884dff\"><i>sequences</i></font> in the <font color=\"blue\">train</font> set and the <font color=\"#ff8005\">test</font> set will have a common shape `(input_length, n_features)` $ = (14\\times8,19) = (112,19)$.\n",
    "\n",
    "üëâ Each <font color=\"#884dff\"><i>sequence</i></font> has a target, the shape of which will be `(output_length, n_targets)` $ = (7\\times8, 1) = (56, 1)$.\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/shapes_for_input_sequence_and_ouput_sequence.png\" alt=\"3d arrays time series\" width=\"1200\" height=\"800\"> \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üî• Open this [**infograph**](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/big_picture_temporal_data_handling.png)  side-by-side with the notebook for a visual summary! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Adapting the functions from the _\"Predict Temperature\"_ challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.0) Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåê Let's define some ***global variables*** that we will use for our tests everywhere in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folds\n",
    "FOLD_LENGTH = 3*365*8      # each fold will have 3 years of data\n",
    "FOLD_STRIDE = int(365/2)*8 # sliding each semester\n",
    "\n",
    "# Temporal Train-Test split\n",
    "TRAIN_TEST_RATIO = 0.66    \n",
    "N_TRAIN = 6666 # number_of_sequences_train for each fold_train\n",
    "N_TEST =  3333 # number_of_sequences_test for each fold_test\n",
    "\n",
    "# Inputs\n",
    "N_FEATURES = 19\n",
    "INPUT_LENGTH = 14*8 # - Records every 3 hours x 8 = 24 hours \n",
    "                    # - During two weeks, which is quite common for weather forecasts  \n",
    "\n",
    "# Outputs\n",
    "TARGET = ['T (degC)']\n",
    "TARGET_COLUMN_IDX = 1 # 'T (degC)' corresponds to the second column of the df\n",
    "N_TARGETS = 1\n",
    "OUTPUT_LENGTH = N_TARGETS*7*8 # - Predicting one target, the temperature \n",
    "                              # - for one week with predictions every 3 hours\n",
    "\n",
    "# Additional parameters\n",
    "HORIZON = 7*8 # - We are predicting the week after the next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1)  üóÇ <font color=\"#c91ac9\">FOLDS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ **`get_folds`**\n",
    "\n",
    "You have already coded this function in *Challenge 3 - Predict Weather Temperature*. <br> _Feel free to ask the teacher any questions if something is still not clear._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(df: pd.DataFrame, \n",
    "              fold_length: int,\n",
    "              fold_stride: int) -> List[pd.DataFrame]:\n",
    "    '''\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "    \n",
    "    Returns a list of folds, each as a DataFrame\n",
    "    '''\n",
    "\n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Let's generate these <font color=\"#c91ac9\">**FOLDS**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = get_folds(df, FOLD_LENGTH, FOLD_STRIDE)\n",
    "\n",
    "print(f'The function generated {len(folds)} folds.')\n",
    "print(f'Each fold has a shape equal to {folds[0].shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Make sure that the following <font color=green>***assert***</font> doesn't return anything (which means the assertion is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(folds[0].shape == (8760, 19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è This amount of <font color=\"#c91ac9\">**FOLDS**</font> should be enough to cross-validate our model correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) ‚úÇÔ∏è  Temporal <font color=blue>Train</font>/<font color=\"#ff8005\">Test</font> Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ Let's <font color=\"#c91ac9\">focus on one fold</font> for the moment, the first one for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = folds[0]\n",
    "fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚åöÔ∏è We want to ***split this <font color=\"#c91ac9\">fold</font> chronologically*** into a <font color=blue>***fold_train***</font> and a <font color=\"#ff8005\">***fold_test***</font>.\n",
    "\n",
    "*Each of these fold_train and fold_test will contain all the data we need to be able to sample many pairs `(Xi, yi)` in a next step!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) üßëüèª‚Äçüè´ The complexity introduced by the <font color=green>gap</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>‚è∏ Let's take a <i>quick break</i> from this weather dataset to understand what is a temporal gap.</summary>\n",
    "\n",
    "<hr>\n",
    "\n",
    "ü§ë Put yourself in the shoes of a quantitative trader at timesteps $\\color{blue}{1}, \\color{blue}{2}, \\color{blue}{3}, ..., \\color{blue}{10}$.\n",
    "\n",
    "Let's say you are in the <font color=blue>training</font> phase, and you want to stop your training after day `10`:\n",
    "- You have to wait until day `10` to know the real value $y_{10}$, and compare it with the predicted value $\\hat{y}_{10}$ to train the model.\n",
    " \n",
    "Here are the <font color=blue>assumptions about your model</font>:\n",
    "- It is trained on <font color=\"#884dff\"><i>sequences</i></font> with `INPUT_LENGTH = 3`\n",
    "- The goal is to predict `OUTPUT_LENGTH = 1` point in the future\n",
    "- You want to predict this point `HORIZON = 4` days after the last known value.\n",
    "\n",
    "‚úÖ Imagine that your model was trained and put into production\n",
    "\n",
    "‚ùì <u>On which day can you evaluate the live performance of the model for the first time</u> ‚ùì\n",
    "\n",
    "* You are at day `10`. Hence, the first prediction you can make is for day `10` + `HORIZON` = `14`\n",
    "* You will have to wait until day `14` to see how good was your prediction!\n",
    "    * `y_test_first` = day `14`\n",
    "* You sit IDLE for <font color=green>3</font> days $(11, 12, 13)$\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/train_test_split_with_horizon.jpg?raw=true\" height=500 width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ The <font color=green>gap</font> between the <font color=blue>train</font> and the <font color=orange>test</font> set should be equal to `GAP = HORIZON - 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) üíª Adapting the  `train_test_split` function accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question ‚ùì (Temporal <font color=blue>Train</font>/<font color=orange>Test</font> split)** \n",
    "\n",
    "Code the function `train_test_split` down below which:\n",
    "- <i>(inputs)</i> given \n",
    "    - a `fold` (like above), \n",
    "    - a `train_test_ratio` (e.g 0.8) \n",
    "    - an `input_length` (fixed)\n",
    "    - üÜï a `horizon` (fixed)\n",
    "- <i>(output)</i> returns a tuple (`fold_train`, `fold_test`) of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(fold: pd.DataFrame, \n",
    "                     train_test_ratio: float,\n",
    "                     input_length: int, \n",
    "                     horizon: int) -> Tuple[pd.DataFrame]:\n",
    "    '''\n",
    "    Returns a train dataframe and a test dataframe (fold_train, fold_test)\n",
    "    from which one can sample (X,y) sequences.\n",
    "    df_train should contain all the timesteps until round(train_test_ratio * len(fold))   \n",
    "    '''\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question ‚ùì (Temporal <font color=blue>Train</font>/<font color=orange>Test</font> split for <font color=\"#c91ac9\">one fold </font>)** \n",
    "\n",
    "Split the <font color=\"#c91ac9\">fold #0</font>.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><i>Reminders</i></summary>\n",
    "\n",
    "*As a reminder, in  section `(2.0) Global Variables`, we defined*:\n",
    "- *`TRAIN_TEST_RATIO` = 66%* \n",
    "- *`INPUT_LENGTH` = 2 weeks = 112 time steps for each `Xi`, which is quite common in weather forecasting*\n",
    "- *`HORIZON` = 1 day = 8 time steps*\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Check that your shapes and your starting/ending points are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double-checking the shapes for the fold_train and fold_test\n",
    "fold_train.shape == (5782, 19)\n",
    "fold_test.shape == (3090, 19)\n",
    "\n",
    "## Double-checking the starting point and the ending point for both folds\n",
    "assert (fold_train.index.start) == 0\n",
    "assert (fold_train.index.stop) == 5782\n",
    "assert (fold_test.index.start) == 5670\n",
    "assert (fold_test.index.stop) == 8760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) üíª üî¢ Create (X, y) sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "print(\"##### INPUTS #####\")\n",
    "print(f'- INPUT_LENGTH = {INPUT_LENGTH} timesteps = {int(INPUT_LENGTH/8)} days = {int(INPUT_LENGTH/8/7)} weeks')\n",
    "print(f'- N_FEATURES = {N_FEATURES}')\n",
    "# Outputs\n",
    "print(\"##### OUTPUTS #####\")\n",
    "print(f'- OUTPUT_LENGTH = {OUTPUT_LENGTH} timesteps = {int(OUTPUT_LENGTH/8)} day(s)')\n",
    "print(f'- N_TARGETS = {N_TARGETS}')\n",
    "# Parameters\n",
    "print(\"##### PARAMETERS #####\")\n",
    "print(f'- HORIZON = {HORIZON} timesteps = {int(HORIZON/8)} day(s)')\n",
    "# Train\n",
    "print(\"##### TRAIN SET #####\")\n",
    "print(f\"- The training fold starts at index {fold_train.index.start} and stops at index {fold_train.index.stop}.\")\n",
    "# Test\n",
    "print(\"##### TEST SET #####\")\n",
    "print(f\"- The test fold starts at index {fold_test.index.start} and stops at index {fold_test.index.stop}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Scanning  through a fold  \n",
    "STRIDE = 3*8 # sliding every three days, for instance\n",
    "print(f'STRIDE = {STRIDE} timesteps = {int(STRIDE/8)} day(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question ‚ùì (Extracting a <font color=\"#884dff\"><i>sequence</i></font>)** \n",
    "\n",
    "Code the function `get_Xi_yi` which extracts a <font color=\"#884dff\"><i>sequence</i></font> from a Time Series: it should take the following arguments:\n",
    "- `first_index`\n",
    "- `data` (your 2D-dataframe representing the Time Series)\n",
    "- `input_length`\n",
    "- `output_length`\n",
    "- `horizon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xi_yi(first_index: int, \n",
    "              fold: pd.DataFrame, \n",
    "              horizon: int,\n",
    "              input_length: int,\n",
    "              output_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    - extracts one sequence from a fold\n",
    "    - returns a pair (Xi, yi) with:\n",
    "        * len(Xi) = `input_length` and Xi starting at first_index\n",
    "        * len(yi) = `output_length`\n",
    "        * last_Xi and first_yi separated by the gap = horizon -1\n",
    "    '''\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the following cell to test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing your function get_Xi_yi\n",
    "\n",
    "first_index = fold.index.start\n",
    "Xi, yi = get_Xi_yi(first_index=first_index,\n",
    "                   fold=fold,\n",
    "                   horizon=HORIZON,\n",
    "                   input_length=INPUT_LENGTH,\n",
    "                   output_length=OUTPUT_LENGTH)\n",
    "\n",
    "assert (Xi.index.start == first_index)\n",
    "assert (Xi.shape == (INPUT_LENGTH, 19))\n",
    "assert (yi.index.stop == Xi.index.stop + HORIZON - 1 + OUTPUT_LENGTH)\n",
    "assert (yi.shape == (OUTPUT_LENGTH, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question ‚ùì (Creating <font color=\"#884dff\"><i>sequences</i></font>, scanning chronologically through a fold)** \n",
    "\n",
    "Code the function `get_X_y` to scan an entire fold and extract sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(fold: pd.DataFrame,\n",
    "            horizon: int,\n",
    "            input_length: int,\n",
    "            output_length: int,\n",
    "            stride: int,\n",
    "            shuffle=True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    - Uses `data`, a 2D-array with axis=0 for timesteps, and axis=1 for (targets+covariates columns)\n",
    "    - Returns a Tuple (X,y) of two ndarrays :\n",
    "        * X.shape = (n_samples, input_length, n_covariates)\n",
    "        * y.shape =\n",
    "            (n_samples, output_length, n_targets) if all 3-dimensions are of size > 1\n",
    "            (n_samples, output_length) if n_targets == 1\n",
    "            (n_samples, n_targets) if output_length == 1\n",
    "            (n_samples, ) if both n_targets and lenghts == 1\n",
    "    - You can shuffle the pairs (Xi,yi) of your fold\n",
    "    \"\"\"\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question ‚ùì Generate <font color=\"#884dff\"><i>sequences</i></font>** both in the <font color=blue>**train**</font> set and the <font color=\"#ff8005\">**test**</font> set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test that your shapes are correct. If not, go back to the function to debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASSERTS\n",
    "assert(X_train.shape == (232, INPUT_LENGTH, N_FEATURES))\n",
    "assert(y_train.shape == (232, OUTPUT_LENGTH))\n",
    "assert(X_test.shape == (120, INPUT_LENGTH, N_FEATURES))\n",
    "assert(y_test.shape == (120, OUTPUT_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) üíª Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) üíª LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Question (the LSTM model)** ‚ùì (<i>Adapting the code from Challenge 3</i>)\n",
    "\n",
    "We took the architecture of the LSTM model from the previous challenge. The function `init_model` initializes and compiles it.\n",
    "\n",
    "üëâ Now that we are ***predicting multiple steps in the future***, what do you need to change in this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def init_model(X_train, y_train):\n",
    "\n",
    "    # 0 - Normalization\n",
    "    # ======================    \n",
    "    normalizer = Normalization()\n",
    "    normalizer.adapt(X_train)\n",
    "    \n",
    "    # 1 - RNN architecture\n",
    "    # ======================    \n",
    "    model = models.Sequential()    \n",
    "    # Normalizing Inputs\n",
    "    model.add(normalizer)\n",
    "    # Recurrent Layer\n",
    "    model.add(layers.LSTM(units=64, activation='tanh', return_sequences=False, \n",
    "                          recurrent_dropout=0.5, dropout=0.5))\n",
    "    # Hidden Dense Layer that we are regularizing\n",
    "    reg_l2 = regularizers.L2(0.5)\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer = reg_l2))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    \n",
    "    # Predictive Dense Layer\n",
    "    ### model.add(layers.Dense(1, activation='linear'))\n",
    "    ### QUESTION: HOW DO YOU CHANGE THIS PART FOR MULTIPLE STEPS ?    \n",
    "    pass  # YOUR CODE HERE\n",
    "    \n",
    "    # 2 - Compiler\n",
    "    # ======================\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=1000, decay_rate=0.5)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[\"mae\"])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ **Training**: We can re-use the function `fit_model` from the last challenge to <font color=blue>train</font> the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def fit_model(model: tf.keras.Model, verbose=1) -> Tuple[tf.keras.Model, dict]:\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\",\n",
    "                       patience=10,\n",
    "                       mode=\"min\",\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=False,\n",
    "                        batch_size=32,\n",
    "                        epochs=200,\n",
    "                        callbacks=[es],\n",
    "                        verbose=verbose)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ **Visualizing performance:** Feel free to use the `plot_history` function to visualize how your model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    # --- LOSS: MSE --- \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- METRICS:MAE ---\n",
    "    \n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "                        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùì **Questions ‚ùì (Training and Evaluating the LSTM model)**\n",
    "\n",
    "Using your functions `init_model` and `fit_model`:\n",
    "1. **Initialize** your model\n",
    "2. <font color=blue>**Train**</font> it and observe the performances on the train set and the validation set\n",
    "3. <font color=orange>**Evaluate**</font> it on the test set\n",
    "\n",
    "_Feel free to apply regularization techniques in the architecture if the model overfits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) üéÅ Baseline with a horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ (*Reminder*) ***In Time Series, an \"intuitive\" baseline model is to repeat the last seen value as (a) prediction(s) for the future value(s)*** you want to forecast, as illustrated down below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/davydw/public-pictures/blob/main/last_seen_value_with_horizon.png?raw=true\" width = 600 height = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ **The Last Seen Value Baseline Model** \n",
    "\n",
    "Let's go together through the function `last_seen_value_baseline` which:\n",
    "- (_input_) takes a pair $(X, y)$ \n",
    "- (_output_) computes the MAE of the `\"repeated last seen value\"`  baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN_IDX = 1 # the second column of each fold corresponds to the temperature\n",
    "\n",
    "def last_seen_value_baseline(X, y):\n",
    "\n",
    "    # How many values do you want to predict in the future ?\n",
    "    output_length = y.shape[-1]\n",
    "    \n",
    "    # For each sequence, let's consider the last seen value\n",
    "    # and only the temperature column\n",
    "    last_seen_values = X[:,-1, TARGET_COLUMN_IDX].reshape(-1,1)\n",
    "\n",
    "    # We need to duplicate these values as many times as output_length\n",
    "    # The author of this notebook did not know how to do it, so they searched on Stackoverflow\n",
    "    # and found this nice np.repeat in Numpy, which is self-explanatory\n",
    "    repeated = np.repeat(last_seen_values, axis = 1, repeats = output_length)\n",
    "\n",
    "    return np.mean(np.abs(y_test - repeated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üïµüèª‚Äç‚ôÄÔ∏è What is the performance of the \"last seen value baseline model\" on the <font color=orange>**test**</font> set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Evaluation of the Baseline Model\n",
    "# ====================================\n",
    "mae_baseline = last_seen_value_baseline(X_test, y_test)\n",
    "print(f\"- The Baseline MAE on the test set is equal to {round(mae_baseline,2)} Celsius degrees\")\n",
    "\n",
    "# 4 - Comparison with the LSTM model\n",
    "# ====================================\n",
    "print(f\"- The LSTM MAE on the test set is equal to {round(res[1],2)} Celsius degrees\")\n",
    "print(f\"üëâ Improvement/decrease of the LSTM model over the baseline (on this fold for the test set) = : {round((1 - (res[1]/mae_baseline))*100,2)} % üëà\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b><i>Warnings:</i></b></u>\n",
    "\n",
    "* Do you remember the challenge `Predict weather temperature` where we were trying to predict the temperature in three hours, i.e. ***the next data point***? We were able to improve the MAE by 30-40% on the LSTM vs. the baseline for every fold, and the MAE was more or less between 1 and 2 degrees Celsius.\n",
    "\n",
    "* <font color=red>***The more data points in the future we try to predict, the higher your MAE will be!***</font>\n",
    "\n",
    "* *It is more and more frequent to see a 4/5 Celsius degrees - difference between the real temperature in seven days and the currently predicted value!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) üíª Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è Reminders of the global variables in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folds\n",
    "print('##### FOLDS ##### ')\n",
    "print(f'- FOLD_LENGTH = {FOLD_LENGTH} timesteps = {3} years x {365} days x {8} records per day')\n",
    "print(f'- FOLD_STRIDE = {FOLD_STRIDE} timesteps = {(365/2)} days x {8} records per day = sliding each semester')\n",
    "\n",
    "# Chronological Train Test Split\n",
    "print('##### TRAIN TEST SPLIT #####')\n",
    "print(f'- TRAIN_TEST_RATIO = {TRAIN_TEST_RATIO}')\n",
    "print(f'- N_TRAIN = {N_TRAIN}')\n",
    "print(f'- N_TEST = {N_TEST}')\n",
    "\n",
    "# Inputs\n",
    "print('##### INPUTS #####')\n",
    "print(f'- INPUT_LENGTH = {INPUT_LENGTH} timesteps = {int(INPUT_LENGTH/8)} days x {8} records per day = {int(INPUT_LENGTH/8/7)} weeks')\n",
    "print(f'- N_FEATURES = {N_FEATURES}') \n",
    "\n",
    "# Outputs\n",
    "print('##### OUTPUTS #####')\n",
    "print(f'- OUTPUT_LENGTH = {OUTPUT_LENGTH} timesteps = {int(OUTPUT_LENGTH/8)} day(s) x {8} records per day')\n",
    "print(f\"- Trying to predict:{TARGET}\")\n",
    "print(f'- N_TARGETS = {N_TARGETS}') \n",
    "\n",
    "# Parameters\n",
    "print('##### PARAMETERS #####')\n",
    "print(f'- HORIZON = {HORIZON} timesteps = {int(HORIZON/8)} day(s) x {8} records per day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember how many folds do we have ?\n",
    "folds = get_folds(df, FOLD_LENGTH, FOLD_STRIDE)\n",
    "print(f\"WARNING, we have {len(folds)} FOLDS, it may take a long time to run...!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ **Cross-validating the LSTM and the baseline** \n",
    "\n",
    "_Let's go together through the following code_ üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reload the dataset, just in case\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/weather_every_three_hours_engineered.csv\"\n",
    "df = pd.read_csv(url).drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_mae_baseline_model = []\n",
    "list_of_mae_recurrent_model = []\n",
    "    \n",
    "# 1 - Creating FOLDS\n",
    "# =======================================================\n",
    "    \n",
    "folds = get_folds(df, FOLD_LENGTH, FOLD_STRIDE)\n",
    "    \n",
    "for fold_id, fold in enumerate(folds):    \n",
    "    \n",
    "    # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "    # =======================================================    \n",
    "    \n",
    "    (fold_train, fold_test) = train_test_split(fold = fold, \n",
    "                                               train_test_ratio = TRAIN_TEST_RATIO, \n",
    "                                               input_length = INPUT_LENGTH, \n",
    "                                               horizon = HORIZON) \n",
    "    \n",
    "    # 3 - Scanninng fold_train and fold_test for SEQUENCES \n",
    "    # =======================================================       \n",
    "    \n",
    "    X_train, y_train = get_X_y(fold = fold_train, \n",
    "                               horizon = HORIZON, \n",
    "                               input_length = INPUT_LENGTH, \n",
    "                               output_length = OUTPUT_LENGTH, \n",
    "                               stride = STRIDE)\n",
    "    \n",
    "    X_test, y_test = get_X_y(fold_test, \n",
    "                             horizon = HORIZON, \n",
    "                             input_length = INPUT_LENGTH, \n",
    "                             output_length = OUTPUT_LENGTH,\n",
    "                             stride = STRIDE)\n",
    "    \n",
    "    # 4.1 - Baseline Model\n",
    "    # =======================================================\n",
    "    mae_baseline = last_seen_value_baseline(X_test, y_test)\n",
    "    list_of_mae_baseline_model.append(mae_baseline)\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(f\"MAE baseline fold n¬∞{fold_id} = {round(mae_baseline, 2)}\")        \n",
    "    \n",
    "    \n",
    "    # 4.2 - LSTM Model\n",
    "    # =======================================================\n",
    "    \n",
    "    # Initializing the LSTM Model\n",
    "    model = init_model(X_train, y_train)\n",
    "    # Training\n",
    "    model, history = fit_model(model, verbose=0)\n",
    "    # Evaluation\n",
    "    res = model.evaluate(X_test, y_test, verbose = 0)\n",
    "    mae_lstm = res[1]\n",
    "    \n",
    "    list_of_mae_recurrent_model.append(mae_lstm)\n",
    "    \n",
    "    print(f\"MAE LSTM fold n¬∞{fold_id} = {round(mae_lstm, 2)}\")\n",
    "    \n",
    "    # 4.3 - Comparison LSTM vs Baseline for the current fold\n",
    "    # =======================================================\n",
    "    print(f\"üèãüèΩ‚Äç‚ôÇÔ∏è Improvement/Decrease vs. Baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mae_baseline = np.mean(list_of_mae_baseline_model)\n",
    "cv_mae_lstm = np.mean(list_of_mae_recurrent_model)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"Average MAE baseline = {round(cv_mae_baseline, 2)}\")    \n",
    "print(f\"Average LSTM baseline = {round(cv_mae_lstm, 2)}\")    \n",
    "print(f\"üèãüèΩ‚Äç‚ôÇÔ∏è Improvement/Decrease vs. Baseline: {round((1 - (cv_mae_lstm/cv_mae_baseline))*100,2)} % \\n\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations for finishing this day about RNN!\n",
    "\n",
    "üÉè Don't forget your flashcards!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Final words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ü§Ø Predicting the temperature for 7 days every 3 hours, after waiting for 7 days, is not an easy task at all.\n",
    "    - The LSTM model did not perform well on some folds\n",
    "    - 5 degrees is a high MAE, one should take into account how \"volatile\" the temperature has been over the last two decades...\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "* üïµüèª We may need weather data from other cities.\n",
    "* ü§î If you plan to use Recurrent Neural Networks during your project, ask your Batch Mananger and the most experienced teachers and TAs if about the quality of your data and the feasibility of your projects.\n",
    "* üòá Keep the morale high, RNNs are complex networks but they are pretty useful for _Natural Language Processing_. Stay tuned!\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: what is the main difficulty when trying to regress multiple targets in RNN ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Optional question only for very advanced students!*\n",
    "- *For 99% of the students: read it later :) it's time to go home!*\n",
    "\n",
    "üïµüèª‚Äç‚ôÇÔ∏è We could have also tried to predict to <b>predict multiple targets</b>... which would have complexified the problem! \n",
    "\n",
    "<hr>\n",
    "\n",
    "<details>\n",
    "        <summary><i>Why, you ask ?</i></summary>\n",
    "    \n",
    "Imagine that you want to predict `N_TARGETS = 3` for 7 days in the future `OUTPUT_LENGTH = 7 x 8  = 56 timesteps`\n",
    "    \n",
    "- üå° `T (degC)` = temperature in Celsius degrees\n",
    "- ‚ôíÔ∏è `p (mbar)` = atmospheric air pressure in millibars\n",
    "- üí¶ `rh (%)` = relative humidity expressed as a percent \n",
    "    \n",
    "üëç The predictive Dense Layer in your architecture will contain `3 x (7 x 8) = 168` neurons, that is totally doable. \n",
    "    \n",
    "‚ò¢Ô∏è However when you compile the model, you will choose `metrics = ['mae']` and will evaluate your model on a metric which doesn't make any sense because you are mixing absolute errors of temperature, air pressure and relative humidity... ***You should create a custom metrics which computes one mae per target*** for example. It is possible  but for this recap, we will stay focused on one feature. Feel free to think about this question if your data science project needs it! </details>        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
