{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö In this exercise, you will\n",
    "\n",
    "- Code your own gradient descent in vectorized form for a high-dimension loss function\n",
    "- Fine-tune your choice of number of epochs on gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ We are going to study the [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) and try to predict the **intensity of the disease** based on **10 quantitative features** such as body-mass-index, age and others - hence, it's a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y, kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code your vectorial gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're modeling a linear regression $\\hat{y} = X\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/linear_reg_matrix_multiplication.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá So, first, let's add an \"intercept\" column of \"ones\" to our feature matrix `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add an intercept column of \"ones\" \n",
    "X = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We create for you a train test split with `test_size=0.3` and `random_state=1` (for comparable results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's recall the definition of the gradient descent algorithm\n",
    "\n",
    "$$\\text{Gradient descent - vector formula}$$\n",
    "$$\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\beta^{\\color{red}{(k)}})$$\n",
    "\n",
    "The MSE Loss for an OLS regression is\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n}\\|X \\beta - y\\|^2 = \\frac{1}{n}(X \\beta - y)^T(X \\beta - y)$$\n",
    "\n",
    "and its gradient is\n",
    "$${\\displaystyle \\nabla L(\\beta)=\n",
    "{\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\beta_{0}}}(\\beta)\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\beta_{p}}}(\\beta)\\end{bmatrix}} = \\frac{2}{n} X^T (X\\beta - y) \n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Let's store our main problem parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n observations\n",
    "n = X.shape[0] \n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# p features (including intercept)\n",
    "p = X.shape[1]\n",
    "\n",
    "# Gradient Descent hyper-params\n",
    "eta = 0.1\n",
    "n_epochs= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Initialize a $\\beta$ vector of zeros of shape **p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the vectorized formula given above, create a gradient descent that loops over `n_epochs` to find the best. $\\beta$ of an OLS using the `train` set\n",
    "- make use of numpy's matrix operations and broadcasting capabilities\n",
    "- this shouldn't take more than 4 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your gradient descent in less than 4 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute predictions on your test set and store it into the variable `y_pred`, as well as the resulting loss (MSE loss for OLS) and store it into `loss_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap these into a function `gradient_decent`\n",
    "\n",
    "‚ùì Wrap this logic into a function `gradient_descent`, which takes as input training and testing data `X_train`, `y_train`, `X_test`, `y_test`, a learning rate `eta`, and the number of epochs `n_epoch`, and returns:\n",
    "- the final value for $\\beta$ fitted on the train set\n",
    "- the values of the `loss_train` at each epoch as a list `loss_train_history`\n",
    "- the values of the `loss_test` at each epoch as a list `loss_test_history`\n",
    "- (optional) make the function robust to call with only a train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_test, y_test, eta=eta, n_epochs=100):\n",
    "        \n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "    \n",
    "    beta = np.zeros(p)\n",
    "    \n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "    \n",
    "    pass  # YOUR CODE HERE\n",
    "        \n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the defined function, plot the loss as a function of epochs, on your train dataset. \n",
    "- Try it with `n_epochs=10000` and `eta=0.1`\n",
    "- Zoom in with `plt.ylim(ymin=2800, ymax=3000)` to see the behavior of the loss function on the test set\n",
    "- What can you conclude? Should you always \"descend\" down to the absolute minimum? ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> üÜò Answer </summary>\n",
    "    \n",
    "Had we stopped gradient descent earlier, we could have obtained a better test MSE loss. We are probably **overfitting** on our dataset.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCan you think of a method to improve the performance of your model? Take time to write it in pseudo-code below before looking at the hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Hints</summary>\n",
    "\n",
    "- We could decide to stop the gradient descent as soon as the non-train loss starts to increase back up again.\n",
    "- ‚ö†Ô∏è Yet we can't use the \"test set\" created initially to decide when to stop descending down the gradient --> this would create a serious data-leak! Never use your test set to optimize your model `hyperparameters`.\n",
    "- Create instead a train/test split **within** your current training set and optmize your early stopping based on the loss on this new test set only. This set is usually called a **validation set**. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Update your `gradient_descent` method based on the hints above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create your training and validation set with `train_test_split` and try to improve your MSE with early stopping, using `random_state=1` and default test size\n",
    "\n",
    "It should stop earlier than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Modify your `gradient_descent` function into a `minibatch_gradient_descent` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X_train, y_train, X_test, y_test, batch_size=16, eta=eta, n_epochs=n_epochs):\n",
    "\n",
    " \n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "    beta = np.zeros((p,1)) \n",
    "    \n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "                \n",
    "        # Shuffle your (X_train,y_train) dataset\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "        # Loop over your dataset minibatch-per-minibatch, and for each mini-batch update your beta\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "        # keep track of loss histories per epoch\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "    return beta, loss_train_history, loss_test_history\n",
    "\n",
    "beta_mini, loss_train_history_mini, loss_val_history_mini =\\\n",
    "minibatch_gradient_descent(X_train_train, y_train_train, X_val, y_val,\n",
    "                           batch_size=8,\n",
    "                           n_epochs=100)\n",
    "\n",
    "plt.plot(loss_train_history_mini, label='train loss')\n",
    "plt.plot(loss_val_history_mini, label='val less')\n",
    "plt.title('Minibatch loss history')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the evolution of your train data and validation data losses per epoch. What if you choose `batch_size=1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How would you adjust the early stopping criteria to account for these fluctuations?\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "\n",
    "    \n",
    "To avoid early stopping too early due to the stochastic nature of the minibatch descent, we could add a `patience` integer term, so that the algorithm only stops after validation loss increases for a sustained period of `patience` number of epochs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: A new way to check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/new_way_to_check_overfitting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Read more about:\n",
    "\n",
    "- [Underfitting and overfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)\n",
    "- [Gradient Descent in Python](https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ Congrats on completing this challenge! Time to push your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
